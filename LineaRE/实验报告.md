

# **实验报告**

[TOC]



### **一．论文摘要 abstract和 introduction翻译**

#### 1.摘要

> ​		The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, manyto-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two lowdimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.

 		知识图谱的链接预测的任务是预测实体之间的缺失关系。知识图嵌入旨在将知识图的实体和关系表示为连续向量空间中的低维向量，具有良好的预测性能。如果嵌入模型能够尽可能多地涵盖不同类型的连接模式和关系映射属性，那么它可能会为链接预测任务带来更多好处。在本文中，我们提出了一种新的嵌入模型，即 LineaRE ，它能够对四种连接模式（即对称、反对称、反转和组合）和四种映射属性的关系（即一对一、一对多、多对一、多对多）进行建模。具体来说，我们将知识图嵌入视为一个简单的线性回归任务，其中关系被建模为具有两个权重向量和一个偏置向量的两个低维向量用以表示实体的线性函数。由于向量是在实数空间中定义的，并且模型的评分函数是线性的，因此我们的模型很简单，并且可以扩展到大型知识图谱。在多个被广泛使用的现实世界数据集上的实验结果表明，所提出的LineaRE模型在链路预测任务方面明显优于现有的最先进模型。

#### 2.介绍

> ​		The construction and applications of knowledge graphs have attracted much attention in recent years. Many knowledge graphs, such as WordNet [9], DBpedia [10], and Freebase [11], have been built and successfully applied to some AI domains, including information retrieval [12], recommender systems [13], question-answering systems [14], [15], and natural language processing [16]. A large knowledge graph stores billions of factual triplets in the form of directed graphs, where each triplet in the form of (head entity, relation, tail entity) (denoted by (h, r, t) in this paper) stands for an edge with two end nodes in the graph, indicating that there exists a specific relationship between the head and tail entities. However, knowledge graphs still suffer from incompleteness, and link prediction, which predicts relations between entities according to existing triplets, is an important way to knowledge completion [17], [18]. On a graph with this kind of symbolic representation, algorithms that compute semantic relationships between entities usually have high computational complexity and lack scalability. Therefore, knowledge graph embedding is proposed to improve the calculation efficiency. By embedding entities and relations into a low-dimensional vector space, we can efficiently implement the operations such as the calculation of semantic similarity between entities, which is of considerable significance to the completion, reasoning, and applications of knowledge graphs. 
>
> ​		Quite a few methods [1], [6]–[8], [19], [20] have been proposed for knowledge graph embedding. Given a knowledge graph, these methods first assign one or more vectors (or matrices) to each entity and relation, then define a scoring function fr(h, t) to measure the plausibility of each triplet, and finally maximize the global plausibility of all triplets. Thus, scoring functions play a critical role in the methods, which determine the capability and computational complexity of models. The capability of a model is primarily influenced by the variety of connectivity patterns and mapping properties of relations it can model. In a knowledge graph, following [8], we have four connectivity patterns of relations:
>
> - Symmetry. A relation r is symmetric if 
>   $$
>   ∀x, y : r(x, y) ⇒ r(y, x)∀x,
>   $$
>
> - Antisymmetry. A relation r is antisymmetric if 
>   $$
>   ∀x, y : r(x, y) ⇒ ¬r(y, x)
>   $$
>
> - Inversion. Relation r1 is inverse to relation r2 if
>   $$
>   ∀x, y : r1(x, y) ⇒ r2(y, x)
>   $$
>
> - Composition. Relation r1 is composed of relation r2 and relation r3 if 
>   $$
>   ∀x, y, z : r2(x, y) ∧ r3(y, z) ⇒ r1(x, z)
>   $$
>
> Also, following [1], we have four mapping properties of relations: 
>
> - One-to-One (1-to-1). Relation r is 1-to-1 if a head can appear with at most one tail. 
> - One-to-Many (1-to-N). Relation r is 1-to-N if a head can appear with many tails. 
> - Many-to-One (N-to-1). Relation r is N-to-1 if many heads can appear with the same tail.
> - Many-to-Many (N-to-N). Relation r is N-to-N if many heads can appear with many tails. 
>
> We call the latter three relations (i.e., 1-to-N, N-to-1, and Nto-N ) as the complex mapping properties.
>
> ​		If an embedding method could model connectivity patterns and mapping properties as many as possible, it would potentially benefit the link prediction task. This is because methods with stronger modeling ability can preserve more structural information of knowledge graphs, so that the embeddings of entities and relations have more precise semantics. For example, in a link prediction task, a model has learned that the relation Nationality is a Composition of BornIn and LocatedIn. When triplets (Tom, BornIn, New York), (New York, LocatedIn, United States) both hold, it can infer that triplet (Tom, Nationality, United States) holds. Another negative instance is that if a method cannot model N-to-1 mapping property, it probably treats Leonardo DiCaprio and Kate Winslet as the same entity when it reads relations (Leonardo DiCaprio, ActorIn, Titanic) and (Kate Winslet, ActorIn, Titanic). 
>
> ​		In this paper, we proposed a novel method, namely Linear Regression Embedding (LineaRE), which interprets a relation as a linear function of entities head and tail. Specifically, our model represents each entity as a low-dimensional vector (denoted by h or t), and each relation as two weight vectors and a bias vector (denoted by w1 r , w2 r , and br), where h, t, w1 r , w2 r , and br ∈ Rk . Given a golden triplet (h, r, t), we expect the equation w1 r ◦ h + br = w2 r ◦ t, where ◦ denotes the Hadamard (element-wise) product 1 . Tables I & II summarize the scoring functions and the modeling capabilities of some state-of-the-art knowledge graph embedding methods, respectively. Table I shows that, the parameters of ComplEx and RotatE are defined in complex number spaces and those of the others (including our model) are defined in real number spaces. Compared with most of the other models, the scoring function of our LineaRE is simpler. Table II shows that, some of them (such as TransE and RotatE) are better at modeling connectivity patterns but do not consider complex mapping properties. In contrast, some others (TransH and DistMult) are better at modeling complex mapping properties but sacrifice some capability to model connectivity patterns. Our LineaRE has the most comprehensive modeling capability. 
>
> ​		We summarize the main contributions of this paper as follows: 
>
> 1.  We propose a novel method LineaRE for knowledge graph embedding, which is simple and can cover all the above connectivity patterns and mapping properties. 
> 2. We provide formal mathematical proofs to demonstrate the modeling capabilities of LineaRE. 
> 3.  We conduct extensive experiments to evaluate our LineaRE on the task of link prediction on several benchmark datasets. The experimental results show that LineaRE has significant improvements compared with the existing state-of-the-art methods.

​		知识图谱的构建和应用近年来备受关注。许多知识图谱，例如 WordNet [9]、DBpedia [10] 和 Freebase [11]，已经构建并成功应用于一些人工智能领域，包括信息检索 [12]、推荐系统 [13]、问答系统[14]、[15] 和自然语言处理 [16]。一个大型知识图谱以有向图的形式存储了数十亿个事实三元组，其中每个以（头实体，关系，尾实体）形式的三元组（在本文中用（h，r，t）表示）代表一条边图中有两个端节点，表示头尾实体之间存在特定的关系。然而，知识图谱仍然存在不完备的问题，而链接预测是根据现有的三元组预测实体之间的关系，是知识完成的重要方式[17]，[18]。在具有这种符号表示的图上，计算实体之间语义关系的算法通常具有较高的计算复杂度并且缺乏可扩展性。因此，提出了知识图谱嵌入来提高计算效率。通过将实体和关系嵌入到一个低维向量空间中，可以高效地实现计算实体间语义相似度等操作，这对知识图谱的补全、推理和应用具有相当重要的意义。

​		已经提出了很多方法 [1]、[6]-[8]、[19]、[20] 用于知识图嵌入。给定一个知识图谱，这些方法首先为每个实体和关系分配一个或多个向量（或矩阵），然后定义一个评分函数fr(h, t)来衡量每个三元组的合理性，最后最大化所有三元组的全局合理性。因此，评分函数在方法中起着至关重要的作用，它决定了模型的能力和计算复杂性。模型的能力主要受其可以建模的各种连接模式和关系映射属性的影响。在知识图中，以下[8]，我们有四种关系连接模式：

​				• 对称。一个关系r是对称的，如果
$$
∀x, y : r(x, y) ⇒ r(y, x)
$$
​				• 反对称。一个关系r是 反对称的，如果
$$
∀x, y : r(x, y) ⇒ ¬r(y, x)
$$
​				• 反转。关系  与关系  相反，如果
$$
∀x, y : r1(x, y) ⇒ r2(y, x)
$$
​				• 组合。关系  由关系  和关系  组成，如果
$$
∀x, y, z : r2(x, y) ∧ r3(y, z) ⇒ r1(x, z)
$$
​		此外，以下[1]，我们有四个关系的映射属性：

​				• 一对一（1 对1）。如果一个头最多可以出现一个尾，则关系 r 是 1 对 1。

​				• 一对多（1 对 N）。如果一个头可以出现多个尾，则关系 r 是 1 对 N。

​				• 多对一（N对1）。如果许多头部可以出现相同的尾部，则关系 r 是 N 对 1。

​				• 多对多（N 对 N）。如果许多头可以出现许多尾，则关系 r 是 N 对 N。

​		我们将后面三种关系（即1对 N，N对1和 N对 N）称为复杂映射属性。

​		如果嵌入方法可以尽可能多地对连接模式和映射属性进行建模，那么它将大大有利于链接预测任务。这是因为建模能力更强的方法可以保留更多知识图谱的结构信息，从而使实体和关系的嵌入具有更精确的语义。例如，在链接预测任务中，模型了解到关系Nationality是BornIn和LocatedIn的组合。当三元组（Tom，BornIn，New York）和（New York，LocatedIn，United States）都成立时，可以推断出三元组（Tom，Nationality，United States）成立。另一个没有预期结果的例子是，如果一个方法不能对N对1映射属性进行建模，那么当它在读取关系（Leonardo DiCaprio，ActorIn，Titanic）和（Kate Winslet，ActorIn，Titanic）时，它可能会将Leonardo DiCaprio和Kate Winslet视为同一个实体。

​		在本文中，我们提出了一种新方法，即线性回归嵌入（LineaRE），它将关系解释为实体头尾的线性函数。具体来说，我们的模型将每个实体表示为一个低维向量（用h或t表示），每个关系表示为两个权重向量和一个偏置向量（用w1 r , w2 r 和 br 表示），其中h，t，w1 r , w2 r 和br  ∈ Rk  。给定一个黄金三元组   (h, r, t)，我们期望方程 w1 r ◦ h + br = w2 r ◦ t成立，其中 ◦ 表示Hadamard（逐元素）乘积1。表 I 和表 II 分别总结了一些最先进的知识图谱嵌入方法的评分函数和建模能力。表 I 表明ComplEx 和 RotatE 的参数定义在复数空间中，其他（包括LineaRE模型）的参数定义在实数空间中。与大多数其他模型相比，LineaRE 的评分功能更简单。表 II 表明其中一些（例如 TransE 和 RotatE）更擅长建模连接模式，但是是在不考虑复杂的映射属性的前提下。相比之下，其他一些模型（TransH 和 DistMult）在建模复杂映射属性方面做得更好，但其牺牲了一些建模连接模式的能力。我们的 LineaRE 具有最全面的建模能力。

​		我们将本文的主要贡献总结如下：

1. 我们提出了一种用于知识图嵌入的新方法 LineaRE ，该方法简单并且可以涵盖上述所有连接模式和映射属性。

2. 我们提供正式的数学证明来展示 LineaRE 的建模能力。

3. 我们进行了大量的实验来评估我们的 LineaRE 在几个基准数据集上的链接预测任务。实验结果表明，与现有的最先进方法相比， LineaRE 有明显的改进。


### **二．问题描述**

1. 验证LineaRE模型的建模能力可以涵盖论文中提到的所有连接模式（对称关系、反对称关系、反转关系和组合关系）和关系映射属性（1对1、1对多、多对1、多对多）；
2. 使用LineaRE模型分别对四个基准数据集FB15k、WN18、FB15k-237和WN18RR进行链接预测，并且将预测结果与论文中其他已有的对比模型的链接预测结果进行比较，从而验证LineaRE模型的链接预测性能是否优于对比模型。
3. 使用LineaRE模型对数据集FB15k进行链接预测，对LineaRE在不同关系类别上的性能进行分析，从而评估LineaRE模型在连接模式和复杂映射属性方面的表现。

### **三．输入、输出、模型算法描述**

#### 1.输入描述：

​	选择基准数据集FB15K、WN18、FB15k-237、WN18RR，分别对每个数据集进行模型训练与测试。

​	下表为实验所需数据集的统计信息。

> | 数据集    | \# E   | \# R  | \# Train | # Valid | \# Test |
> | --------- | ------ | ----- | -------- | ------- | ------- |
> | FB15k     | 14,951 | 1,345 | 483,142  | 50,000  | 59,071  |
> | WN18      | 40,943 | 18    | 141,442  | 5,000   | 5,000   |
> | FB15k-237 | 14,541 | 237   | 272,115  | 17,535  | 20,466  |
> | WN18RR    | 40,943 | 11    | 86,835   | 3,034   | 3,134   |

- 其中，4个基准数据集描述如下：
  - FB15k 是Freebase [11] 的一个子集。70.22% 的测试三元组 （h， r， t） 可以通过训练集中的直接链接的三元组 （t， r ， h） 推断出来，并且22.37%的测试三元组可以通过两步法或者训练集中的三步路径 （h、 p、 t）推断出来。因此，在FB15k上进行链路预测的关键是对反转和组合模式进行建模；
  - WN18 是WordNet [9] 的一个子集。对WN18链接预测的关键是对反转和对称模式进行建模；
  - FB15k-237 是 FB15k 的一个子集，FB15k-237 删除了反转关系。对FB15k-237链路预测的关键是对组合模式进行建模；
  - WN18RR 是 WN18 的一个子集，WN18RR 删除了反转关系。对WN18RR 链路预测的关键是对对称模式进行建模。


#### 2.输出描述：

##### （1）针对问题描述1的输出描述

- 使用LineaRE分别对四个基准数据集的连接关系（对称关系、反对称关系、反转关系和组合关系）进行建模后测试的结果如下表Ⅰ。

| 数据集    | #Total | #Sym      | #Inv      | #Com      | #Others   |
| :-------- | :----- | --------- | --------- | --------- | --------- |
| FB15k     | 59,071 | 7.34      | **70.22** | **22.37** | 0.06      |
| WN18      | 5,000  | **21.74** | **72.22** | 3.0       | 3.04      |
| FB15k-237 | 20,466 | 0         | 0         | **90.40** | 9.60      |
| WN18RR    | 3,134  | **34.65** | 0.29      | 8.33      | **56.73** |

- 使用LineaRE分别对四个基准数据集的关系映射属性（1对1、1对多、多对1、多对多）进行建模后测试的结果如下表Ⅱ。

| 数据集    | #1-to-1 | #1-to-N   | #N-to-1   | #N-to-N   |
| :-------- | :------ | :-------- | :-------- | :-------- |
| FB15k     | 1.63    | 9.56      | 15.80     | **73.02** |
| WN18      | 0.84    | **36.94** | **39.62** | **22.60** |
| FB15k-237 | 0.94    | 6.32      | **22.03** | **70.72** |
| WN18RR    | 1.34    | 15.16     | **47.45** | **36.06** |

##### （2）针对问题描述2的输出描述

- 使用LineaRE在数据集FB15K和WN18上的链路预测结果如下表Ⅰ。

|          | FB15k  |          |          |          |          | WN18    |          |          |          |          |
| :------- | ------ | -------- | -------- | -------- | -------- | ------- | -------- | -------- | -------- | -------- |
|          | MR     | MRR      | hit@1    | hit@3    | hit@10   | MR      | MRR      | hit@1    | hit@3    | hit@10   |
| TransE   | **35** | .729     | .638     | .798     | .873     | 184     | .798     | .713     | .869     | .949     |
| TransH   | 36     | .731     | .641     | .800     | .873     | 372     | .796     | .717     | .856     | .948     |
| DistMult | 59     | .789     | .730     | .830     | .887     | 496     | .810     | .694     | .922     | .949     |
| ComplEx  | 63     | .809     | .757     | .846     | .894     | 531     | .948     | .945     | .949     | .953     |
| ConvE    | 64     | .745     | .670     | .801     | .873     | 504     | .942     | .935     | .947     | .955     |
| RotatE   | 40     | .786     | .723     | .835     | .884     | 264     | .949     | .943     | .953     | .960     |
| LineaRE  | 36     | **.839** | **.799** | **.864** | **.906** | **170** | **.952** | **.947** | **.955** | **.961** |

- 使用LineaRE在数据集 FB15k-237 和WN18RR的链路预测结果如下表Ⅱ。

|          | FB15k-237 |          |          |          |          | WN18RR   |          |          |          |          |
| -------- | --------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |
|          | MR        | MRR      | hit@1    | hit@3    | hit@10   | MR       | MRR      | hit@1    | hit@3    | hit@10   |
| TransE   | 172       | .334     | .238     | .371     | .523     | 2933     | .196     | .021     | .317     | .529     |
| TransH   | **168**   | .339     | .243     | .375     | .531     | 4736     | .210     | .018     | .387     | .473     |
| DistMult | 301       | .311     | .225     | .341     | .485     | 6580     | .424     | .397     | .433     | .476     |
| ComplEx  | 376       | .313     | .227     | .342     | .486     | 6671     | .446     | .416     | .462     | .503     |
| ConvE    | 246       | .316     | .239     | .350     | .491     | 5277     | .46      | .39      | .43      | .48      |
| RotatE   | 174       | .338     | .245     | .373     | .526     | 3536     | .477     | .429     | .493     | .569     |
| LineaRE  | **168**   | **.353** | **.258** | **.389** | **.545** | **1887** | **.486** | **.445** | **.500** | **.571** |

##### （3）针对问题描述3的输出描述

- 使用LineaRE在数据集FB15k上按关系类别划分的详细链路预测结果如下表Ⅰ。

|          | 1-to-1                | 1-to-N   | N-to-1   | N-to-N   | 1-to-1                | 1-to-N   | N-to-1   | N-to-N   |
| -------- | --------------------- | -------- | -------- | -------- | --------------------- | -------- | -------- | -------- |
| **Task** | **预测头部(Hits@10)** |          |          |          | **预测尾部(Hits@10)** |          |          |          |
| TransE   | .913                  | **.974** | .622     | .880     | .895                  | .705     | **.967** | .908     |
| TransH   | .914                  | .973     | .612     | .883     | .894                  | .680     | **.967** | .910     |
| DistMult | .925                  | .965     | .657     | .890     | .923                  | .821     | .949     | .917     |
| ComplEx  | .928                  | .962     | .673     | .897     | **.934**              | .831     | .950     | .923     |
| RotatE   | **.933**              | .973     | .630     | .894     | .933                  | .709     | .965     | .922     |
| LineaRE  | .926                  | .972     | **.723** | **.905** | .913                  | **.837** | .965     | **.932** |
| **Task** | **预测头部(MRR)**     |          |          |          | **预测尾部 (MRR)**    |          |          |          |
| TransE   | .736                  | .925     | .489     | .721     | .731                  | .582     | .903     | .744     |
| TransH   | .731                  | .922     | .470     | .728     | .730                  | .559     | .905     | .751     |
| DistMult | .813                  | .922     | .526     | .793     | .805                  | .683     | .886     | .817     |
| ComplEx  | .820                  | .928     | .557     | .819     | .815                  | .717     | .890     | .838     |
| RotatE   | **.859**              | **.938** | .511     | .790     | **.857**              | .627     | .906     | .814     |
| LineaRE  | .825                  | **.938** | **.618** | **.842** | .817                  | **.751** | **.919** | **.865** |

 

#### 3.模型算法描述：

##### （1）模型算法描述

- **LineaRE模型逐步训练算法**

  - 损失函数
  - 对于一个黄金三元组（h,r,t），负三元组的权重（即概率分布）为

  ![image-20221021151802326](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021151802326.png)

  其中，α是采样的温度，![image-20221021152417638](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021152417638.png)是（h,r,t)的负三态；

  - 一个观察到的的三元组及其负样本定义逻辑损失函数为

    ![image-20221021152334159](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021152334159.png)

    ![image-20221021152357427](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021152357427.png)

    其中，γ是一个固定的余量，β是一个参数，可以调整正负样本分数之间的余量；λ是正则化系数，E是知识图谱中的实体集合。

```python
def train_step(model, optimizer, data):
   model.train()
   optimizer.zero_grad()
   batch, ht = data
   sample, neg_ents, weight = batch
   ent_reg, rel_reg, pos_loss, neg_loss = model(sample, weight, ht, neg_ents)
   weight_sum = torch.sum(weight)
   pos_loss = torch.sum(pos_loss) / weight_sum
   neg_loss = torch.sum(neg_loss) / weight_sum
   loss = (pos_loss + neg_loss) / 2
   loss += torch.cat([ent_reg ** 2, rel_reg ** 2]).mean() * config.regularization
   loss.backward()
   optimizer.step()
   log = {
      'ent_reg': ent_reg.mean().item(),
      'rel_reg': rel_reg.mean().item(),
      'pos_sample_loss': pos_loss.item(),
      'neg_sample_loss': neg_loss.item(),
      'loss': loss.item()
   }
   return log
```

- **LineaRE模型逐步测试算法**

  - 分别使用MR、MRR、HITS@1、HITS@3、HITS@10指标对测试结果进行评估。

  - LineaRE的评分函数为

    ![image-20221021152801710](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021152801710.png)

    其中，![image-20221021173127590](C:\Users\H\AppData\Roaming\Typora\typora-user-images\image-20221021173127590.png)表示向量x的N1标准。

- 

```python
def test_step(model, test_dataset_list, detail=False):
   def get_result(ranks_):
      return {
         'MR': np.mean(ranks_),
         'MRR': np.mean(np.reciprocal(ranks_)),
         'HITS@1': np.mean(ranks_ <= 1.0),
         'HITS@3': np.mean(ranks_ <= 3.0),
         'HITS@10': np.mean(ranks_ <= 10.0),
      }

   model.eval()
   mode_ents = {'head-batch': 0, 'tail-batch': 2}
   step = 0
   total_step = sum([len(dataset[0]) for dataset in test_dataset_list])
   ranks = []
   mode_rtps = []
   metrics = []
   with torch.no_grad():
      for test_dataset, mode in test_dataset_list:
         rtps = []
         for pos_sample, filter_bias, rel_tp in test_dataset:
            pos_sample = pos_sample.to(config.device)
            filter_bias = filter_bias.to(config.device)
            sort = model(pos_sample, filter_bias, mode)
            true_ents = pos_sample[:, mode_ents[mode]].unsqueeze(dim=-1)
            batch_ranks = torch.nonzero(torch.eq(sort, true_ents), as_tuple=False)
            ranks.append(batch_ranks[:, 1].detach().cpu().numpy())
            rtps.append(rel_tp)
            if step % config.test_log_step == 0:
               logging.info(f'Evaluating the model... ({step:d}/{total_step:d})')
            step += 1
         mode_rtps.append(rtps)
      ranks = np.concatenate(ranks).astype(np.float32) + 1.0
      result = get_result(ranks)
      if not detail:
         return result
      metrics.append(result)
      mode_ranks = [ranks[:ranks.size // 2], ranks[ranks.size // 2:]]
      for i in range(2):
         mode_ranks_i = mode_ranks[i]
         rtps = np.concatenate(mode_rtps[i])
         for j in range(1, 5):
            ranks_tp = mode_ranks_i[rtps == j]
            result = get_result(ranks_tp)
            metrics.append(result)
   return metrics
```

##### （2）框架图

![框架图](C:\Users\H\Desktop\框架图.png)

### **四．评价指标及其计算公式**

#### 1.MR

- 指标说明
  - MR的全称是Mean Rank，即平均排名。
- 计算方法
  - 对于每个评测三元组，移去头部实体（迭代的方式替换尾部实体）、轮流替换成词表中的其他实体，构建错误的三元组实体。利用关系函数计算头部实体和尾部实体的相似度。对于这个相似度来讲，正确的三元组的值应该比较小，而错误样本的相似度值会比较大。用关系函数对所有的三元组（包括正确的三元组和错误的三元组）进行计算，并按照升序排序。最后找出所有正确三元组在该排序中的排名位置。对于一个好的知识图谱表示来说，正确三元组的得分（即头部实体和尾部实体的关系函数值）会小于错误三元组的得分，排名会比较靠前，所以可以用Mean Rank的方式衡量知识图谱表示向量的好坏程度。
- 计算公式
  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/9de83f8c3c0a4e0da65e88945711ca16.png)
  - 其中S是三元组集合，|S|是三元组集合个数，rank_i是指第i个三元组的链接预测排名。该指标越小越好。

#### 2.MRR

- 指标说明
  - MRR的全称是Mean Reciprocal Ranking，即指多个查询语句的排名倒数的均值。
- 计算方法
  - 如果MRR的第一个结果匹配，分数为1，第二个匹配分数为0.5，第n个匹配分数为1/n；如果没有匹配的句子，则分数为0。最终的分数为所有得分之和。
- 计算公式
  - ![MRR计算公式](https://img-blog.csdnimg.cn/fbeb3195176b45daad0219bdbce5e1ff.png)
  - 其中S是三元组集合，|S|是三元组集合个数，rank_i是指第i个三元组的链接预测排名。该指标越大越好。

#### 3.HITS@n

- 指标说明
  - HITS@n是指在链接预测中排名小于n的三元组的平均占比。
- 计算方法
  - HITS@n的第一个正确答案的排名小于n为1，否则为0；全部累加求平均值。一般地，取n值等于1、3或者10。
- 计算公式
  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/20dca2c5b4c54d6e9fa9b3c4a5c32325.png)
  - 其中S是三元组集合，|S|是三元组集合个数，另外Ⅱ(.)是indicator函数（若条件真则函数值为1，否则为0)。该指标越大越好。



### **五．对比方法及引用出处**

#### 1.	对比模型及引用文献序号

- **translational distance models（平移距离模型）**

  - **TransE [1] [8]**

    > TransE是最经典的平移距离模型，RotatE 可以对所有连接模式进行建模，但是是在不考虑复杂的关系映射属性的前提下。

  - **TransH [2]**

    > TransH被提出来解决TransE在建模复杂映射属性中的问题。

  - **TransR [3]**

    > TransR不能应用于大规模的知识图谱。

  - **TransD [4]**

    > TransD是 TransR的改进。

- **semantic matching model [21]（语义匹配模型）**

  - ###### **Bilinear Models（双线性模型）**

    - **RESCAL [22]**

      > RESCAL是一种经典的双线性模型，但是存在两个严重的缺陷：容易过度拟合，不能应用于大规模的知识图谱。

    - **DisMult [5]**

      > DisMult简化了 RESCAL，但是 DisMult只能处理对称关系。

    - **ComplEx [6]**

      > ComplEx提出通过引入复值嵌入来解决DistMult在处理反对称关系建模中的问题，但是ComplEx无法对组合模式进行建模，并且模型的空间和时间复杂度大大增加。

  - ###### **Neural Network Models（神经网络模型）**

    - **SME [23]**
  
      > SME采用两个线性网络分别捕获二元组的语义，然后通过两个网络输出的内积来衡量整个三元组的合理性。
  
    - **ConvE [7]**
  
      > ConvE是一个多层网络卷积模型。
    
    - **MLP [24]**
    
      > 非线性全连接神经网络
    
    - **NTN [25]**
    
      > 非线性全连接神经网络

#### 2.LineaRE与其他方法的区别

-  相同点：
    - 从评分函数的形式来看，LineaRE在形式上类似于 TransE，都属于平移距离模型；


- 不同点：

    - LineaRE与 TransE的其他变体模型有着本质的不同，例如 TransH、TransR、TransD，以TransR为例，其假设实体具有多个方面，各种关系可能集中在实体的不同方面，TransR的实体嵌入首先通过关系特定的投影矩阵从实体空间投影到对应的关系空间，然后通过平移向量进行连接，而LineaRE是将实体投影到平面或特定向量空间中。

    - LineaRE可以将知识图嵌入视为线性回归任务，甚至可以把TransE看成是一个线性回归任务，其中斜率固定为1，但是TransH 、TransR 、TransD不能。


#### 3.LineaRE与对比方法总结表格

##### （1）LineaRE与其他知识图谱嵌入模型的评分函数计算原理总结

> | Model               | Scoring function *fr*(*h*,*h*)                     | # Parameters                       | Venue   |
> | ------------------- | -------------------------------------------------- | ---------------------------------- | ------- |
> | TransE [2]          | ∥*h*+*r*−*t*∥1/2                                   | *h*,*t*,*r*∈R*k*                   | NeurIPS |
> | TransH [15]         | ∥(*h*−*(w*⊤*r)h(wr)*)+*dr*−(*t*−(*w*⊤*r)t(wr)*∥2 2 | *h*,*t*,*wr*,*dr*∈R*k*             | AAAI    |
> | DistMult [18]       | （*h*⊤)*diag*(*r*)*t*                              | *h*,*t*,*r*∈R*k*                   | ICLR    |
> | ComplEx [12]        | *Re*(*(h*⊤)*diag*(*r*)¯*t*)                        | *h*,*t*,*r*∈C*k*                   | ICML    |
> | ConvE [4]           | <*σ*(*vec*(*σ*([¯*r*,¯*h*]∗*Ω*))*W*),*t*>          | *h*,*t*,*r*∈R*k*                   | AAAI    |
> | RotatE [10]         | ∥*h*∘*r*−*t*∥1                                     | *h*,*t*,*r*∈C*k*,\|*ri*\|=1        | ICLR    |
> | LineaRE (Our model) | ∥*(w r 1)∘*h*+b*r*−*(w r 2)*∘*t*∥1                 | *h*,*t*,b *r*,*w r 1*,*w r 2*∈R*k* | –       |

##### （2）LineaRE与其他知识图谱嵌入模型在对称关系、反对称关系、反转关系、组合关系和复杂的映射属性方面的建模能力总结
>
> | Model               | Symmetry | Antisymmetry | Inversion | Composition | Complex mapping properties |
> | ------------------- | -------- | ------------ | --------- | ----------- | -------------------------- |
> | TransE [2]          | –        | ✓            | ✓         | ✓           | –                          |
> | TransH [15]         | ✓        | ✓            | –         | –           | ✓                          |
> | DistMult [18]       | ✓        | –            | –         | –           | ✓                          |
> | ComplEx [12]        | ✓        | ✓            | ✓         | –           | ✓                          |
> | RotatE [10]         | ✓        | ✓            | ✓         | ✓           | –                          |
> | LineaRE (Our model) | ✓        | ✓            | ✓         | ✓           | ✓                          |

### **六．结果**

##### 1.针对问题描述1的结果

- 使用LineaRE模型分别对四个基准数据集FB15K、WN18、FB15k-237、WN18RR进行建模，通过表格数据[实验报告三.1.(1)表Ⅰ和表Ⅱ]可知，LineaRE模型能够覆盖数据集的主要连接模式和映射属性。

##### 2.针对问题描述2的结果

- 对比FB15k和WN18数据集的链路预测结果，通过表格数据[实验报告三.2.(2)表Ⅰ]可知，除了TransX在FB15k数据集上的MR指标表现略好于LineaRE以外，LineaRE在几乎所有的指标上都明显优于其他对比模型。
- 对比FB15k-237和WN18RR数据集的链路预测结果，通过表格数据[实验报告三.2.(2)表Ⅱ]可知，LineaRE的链路预测性能在所有指标上的表现都优于其他对比模型。

##### 3.针对问题描述3的结果

- 使用LineaRE模型对数据集FB15k进行链路预测，并将实验结果按照FB15k的不同关系类别进行划分得到详细的结果[实验报告六.2.(3)表Ⅰ]，表格表明DistMult、ComplEx和LineaRE模型都能够对复杂的映射属性进行建模，并且三个模型在1对多、多对1和多对多的关系上都取得了良好的性能，同时LineaRE在对复杂关系的建模中具有最佳的性能，而RotatE和TransE的结果表现较差。

##### 4.结论

- 结合[实验报告六.1]和[实验报告六.2]的结论可以发现，如果一个模型能够覆盖一个数据集的主要连接模式和映射属性，那么它在该数据集上的链路预测性能表现将会更加良好。以ComplEx和LineaRE为例，两个模型都覆盖了反转关系和复杂的映射属性，同时它们在FB15k数据集和WN18数据集上都取得了良好的链路预测性能结果。
- LineaRE通过线性回归的方式对四种连接模式和四种关系的映射属性进行建模，实验结果表明在几个广泛使用的基准数据集上，LineaRE模型明显由于之前的对比模型。



###### **参考文献**

> [1] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, “Translating embeddings for modeling multi-relational data,” in Advances in Neural Information Processing Systems, 2013, pp. 2787–2795.
>
> [2] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge graph embedding by translating on hyperplanes,” in Proceedings of the 28th AAAI Conference on Artificial Intelligence, 2014.
>
> [3] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2015.
>
> [4] G. Ji, S. He, L. Xu, K. Liu, and J. Zhao, “Knowledge graph embedding via dynamic mapping matrix,” in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015, pp. 687–696.
>
> [5] B. Yang, W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding entities and relations for learning and inference in knowledge bases,” arXiv preprint arXiv:1412.6575, 2014.
>
> [6] T. Trouillon, J. Welbl, S. Riedel, E. Gaussier, and G. Bouchard, ´ “Complex embeddings for simple link prediction,” in Proceedings of the 33rd International Conference on Machine Learning, 2016, pp. 2071– 2080.
>
> [7] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel, “Convolutional 2d knowledge graph embeddings,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018.
>
> [8] Z. Sun, Z.-H. Deng, J.-Y. Nie, and J. Tang, “Rotate: Knowledge graph embedding by relational rotation in complex space,” in Proceedings of the 7th International Conference on Learning Representations. OpenReview.net, 2019.
>
> [9] G. A. Miller, “Wordnet: a lexical database for english,” Communications of the ACM, vol. 38, no. 11, pp. 39–41, 1995.
>
> [10] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. Van Kleef, S. Auer et al., “Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia,” Semantic Web, vol. 6, no. 2, pp. 167–195, 2015.
>
> [11] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, “Freebase: a collaboratively created graph database for structuring human knowledge,” in Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. ACM, 2008, pp. 1247–1250.
>
> [12] C. Xiong, R. Power, and J. Callan, “Explicit semantic ranking for academic search via knowledge graph embedding,” in Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2017, pp. 1271– 1279.
>
> [13] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma, “Collaborative knowledge base embedding for recommender systems,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016, pp. 353–362.
>
> [14] Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu, and J. Zhao, “An end-to-end model for question answering over knowledge base with cross-attention combining global knowledge,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp. 221–231.
>
> [15] X. Huang, J. Zhang, D. Li, and P. Li, “Knowledge graph embedding based question answering,” in Proceedings of the 12th ACM International Conference on Web Search and Data Mining. ACM, 2019, pp. 105–113.
>
> [16] B. Yang and T. Mitchell, “Leveraging knowledge bases in lstms for improving machine reading,” arXiv preprint arXiv:1902.09091, 2019.
>
> [17] Z. Sun, W. Hu, Q. Zhang, and Y. Qu, “Bootstrapping entity alignment with knowledge graph embedding.” in Proceedings of the 27th International Joint Conference on Artificial Intelligence, 2018, pp. 4396–4402.
>
> [18] S. Jia, Y. Xiang, X. Chen, and K. Wang, “Triple trustworthiness measurement for knowledge graph,” in The World Wide Web Conference, 2019, pp. 2865–2871.
>
> [19] Y. Lin, Z. Liu, H. Luan, M. Sun, S. Rao, and S. Liu, “Modeling relation paths for representation learning of knowledge bases,” in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 705–714.
>
> [20] S. M. Kazemi and D. Poole, “Simple embedding for link prediction in knowledge graphs,” in Advances in Neural Information Processing Systems, 2018, pp. 4284–4295.
>
> [21] Q. Wang, Z. Mao, B. Wang, and L. Guo, “Knowledge graph embedding: A survey of approaches and applications,” IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 12, pp. 2724–2743, 2017.
>
> [22] M. Nickel, V. Tresp, and H.-P. Kriegel, “A three-way model for collective learning on multi-relational data.” in Proceedings of the 28th International Conference on Machine Learning, vol. 11, 2011, pp. 809– 816.
>
> [23] A. Bordes, X. Glorot, J. Weston, and Y. Bengio, “A semantic matching energy function for learning with multi-relational data application to word-sense disambiguation,” Machine Learning, vol. 94, no. 2, pp. 233– 259, 2014.
>
> [24] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang, “Knowledge vault: A web-scale approach to probabilistic knowledge fusion,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 601–610.
>
> [25] R. Socher, D. Chen, C. D. Manning, and A. Ng, “Reasoning with neural tensor networks for knowledge base completion,” in Advances in Neural Information Processing Systems, 2013, pp. 926–934.
>
> [26] L. Cai and W. Y. Wang, “Kbgan: Adversarial learning for knowledge graph embeddings,” arXiv preprint arXiv:1711.04071, 2017.
>
> [27] Y. Zhang, Q. Yao, Y. Shao, and L. Chen, “Nscaching: simple and efficient negative sampling for knowledge graph embedding,” in Proceedings of the 35th IEEE International Conference on Data Engineering. IEEE, 2019, pp. 614–625.
>
> [28] P. Wang, S. Li, and R. Pan, “Incorporating gan for negative sampling in knowledge representation learning,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018.
>
> [29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
>
> [30] K. Toutanova and D. Chen, “Observed versus latent features for knowledge base and text inference,” in Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, 2015, pp. 57–66.
>
> [31] G. Bouchard, S. Singh, and T. Trouillon, “On approximate reasoning capabilities of low-rank vector spaces,” AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR), 2015.
>
> [32] M. Nickel, L. Rosasco, and T. Poggio, “Holographic embeddings of knowledge graphs,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016. 